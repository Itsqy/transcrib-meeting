#!/bin/bash
# Check Ollama installation and service status

echo "üîç Checking Ollama..."
echo ""

# 1. Check if ollama command exists
if ! command -v ollama &> /dev/null; then
    echo "‚ùå Ollama not installed"
    echo ""
    echo "üì¶ Installation options:"
    echo "   ‚Ä¢ macOS (Homebrew):  brew install ollama"
    echo "   ‚Ä¢ Or download:       https://ollama.ai/download"
    echo ""
    echo "üí° Quick setup: Run ./scripts/setup_ollama.sh"
    exit 1
fi

echo "‚úÖ Ollama installed: $(ollama --version)"
echo ""

# 2. Check if service is running
if ! curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
    echo "‚ö†Ô∏è  Ollama service not running"
    echo ""
    echo "üöÄ Start the service:"
    echo "   ollama serve"
    echo ""
    echo "üí° Or run in background:"
    echo "   nohup ollama serve > /tmp/ollama.log 2>&1 &"
    exit 1
fi

echo "‚úÖ Ollama service running on http://localhost:11434"
echo ""

# 3. Check if model is available
MODEL=${1:-llama3.1}
if ! ollama list | grep -q "$MODEL"; then
    echo "‚ö†Ô∏è  Model '$MODEL' not found"
    echo ""
    echo "üì• Download the model:"
    echo "   ollama pull $MODEL"
    echo ""
    echo "üí° Other model options:"
    echo "   ‚Ä¢ llama3.1       (recommended - 4.7GB)"
    echo "   ‚Ä¢ llama3.1:8b    (smaller - 4.7GB)"
    echo "   ‚Ä¢ phi3           (faster - 2.2GB)"
    echo "   ‚Ä¢ llama3.1:70b   (better quality - 40GB)"
    exit 1
fi

echo "‚úÖ Model '$MODEL' available"
echo ""

# 4. Test API connection
echo "üß™ Testing API connection..."
if curl -s http://localhost:11434/api/generate -d '{"model":"'$MODEL'","prompt":"test","stream":false}' | grep -q "response"; then
    echo "‚úÖ API responding correctly"
else
    echo "‚ö†Ô∏è  API test failed"
    exit 1
fi

echo ""
echo "üéâ Ollama is ready!"
echo ""
echo "üìù Configuration:"
echo "   ‚Ä¢ Provider: local"
echo "   ‚Ä¢ Model: $MODEL"
echo "   ‚Ä¢ Base URL: http://localhost:11434"
echo ""
echo "‚ú® You can now use free AI summarization!"
echo ""
echo "Test with:"
echo "   python scripts/cli.py summarize transcript.txt --provider local"
